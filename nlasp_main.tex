% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% LaTeX4EI Template for Cheat Sheets                                Version 1.0
%
% Authors: Emanuel Regnath, Martin Zellner
% Contact: info@latex4ei.de
% Encode: UTF-8, tabwidth = 4, newline = LF
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


% ======================================================================
% Document Settings
% ======================================================================

% possible options: color/nocolor, english/german, threecolumn
% defaults: color, english
\documentclass[english]{latex4ei/latex4ei_sheet}

% set document information
\title{NLASP}
\author{LaTeX4EI}					% optional, delete if unchanged
\myemail{info@latex4ei.de}			% optional, delete if unchanged
\mywebsite{www.latex4ei.de}			% optional, delete if unchanged


\let\T\relax
\DeclareMathOperator{\T}{\textsf{\textit{T}}}		% Zufallsvariable X
\DeclareMathOperator{\Bias}{Bias}		% Zufallsvariable X
\DeclareMathOperator{\argmax}{argmax}

\DeclareMathOperator{\rang}{rang}
\renewcommand{\vec}[1]{\underline{\boldsymbol{#1}}}
%\renewcommand{\vec}[1]{\boldsymbol{#1}}



\newcommand{\x}{\textit{x}}
\newcommand{\vx}{\underline{\textbf{\textit{x}}}}
\newcommand{\VX}{\underline{\textbf{\textit{X}}}}

\newcommand{\y}{\textit{y}}
\newcommand{\vy}{\underline{\textbf{\textit{y}}}}
\newcommand{\VY}{\underline{\textbf{\textit{Y}}}}


\newcommand{\VA}{\underline{\textit{A}}}
\newcommand{\A}{\textit{A}}
\newcommand{\va}{\underline{\textbf{\textit{a}}}}

\newcommand{\VB}{\underline{\textit{B}}}


% ======================================================================
% Begin
% ======================================================================
\begin{document}

\IfFileExists{git.id}{\input{git.id}}{}
\ifdefined\GitRevision\mydate{\GitNiceDate\ (git \GitRevision)}\fi
% Title
% ----------------------------------------------------------------------
\maketitle   % requires ./img/Logo.pdf

% Section
% ----------------------------------------------------------------------
\section{Math}
% ======================================================================

\begin{sectionbox}
	\begin{tabular}{@{}llll}
		$\pi \approx \num{3,14159}$ & $e \approx \num{2,71828}$ & $\sqrt{2} \approx \num{1,414}$ & $\sqrt{3} \approx \num{1,732}$ \\
	\end{tabular}

	\textbf{Binome, Trinome}\\
	$(a\pm b)^2 = a^2 \pm 2ab + b^2$ \hfill $a^2 - b^2 = (a-b)(a+b)$\\
	$(a \pm b)^3 = a^3 \pm 3a^2b + 3ab^2 \pm b^3$\\
	$(a+b+c)^2 = a^2 + b^2 + c^2 + 2ab + 2ac + 2bc$
	\\[0.5em]
	\textbf{Folgen und Reihen}\\
	$\underset{\text{Aritmetrische Summenformel}}{\sum \limits_{k=1}^{n} k = \frac{n (n+1)}{2}}$ \quad $\underset{\text{Geometrische Summenformel}}{\sum \limits_{k=0}^{n} q^k = \frac{1 - q^{n+1}}{1-q}}$ \quad $\underset{\text{Exponentialreihe}}{\sum\limits_{n = 0}^{\infty} \frac{\cx z^n}{n!} = e^{\cx z}}$\\
	\\[0.5em]
	\textbf{Mittelwerte} \quad ($\sum$ von $i$ bis $N$) \hfill {\small (Median: Mitte einer geordneten Liste)}\\
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}l@{\quad\ $\ge$}l@{\quad\ $\ge$}l}
	$\underset{\text{Arithmetisches}}{\ol x_{\ir{ar}} = \frac{1}{N} \sum x_i}$ & $\underset{\text{Geometrisches Mittel}}{\ol x_{\ir{geo}} = \sqrt[N]{ \prod x_i }}$ & $\underset{\text{Harmonisches}}{\ol x_{\ir hm} = }\frac{N}{\sum \frac{1}{x_i}}$\\
	\end{tabular*}
	\\[0.5em]
	\textbf{Ungleichungen:} \hfill Bernoulli-Ungleichung:  $(1+x)^n \ge 1+nx$\\
	$\underset{\text{Dreiecksungleichung}}{\big|\! \abs{x}- \abs{y}\!\big| \le \abs{x \pm y} \le \abs{x} + \abs{y}}$ \hfill
	$\underset{\text{Cauchy-Schwarz-Ungleichung}}{\left| \vec x^\top \bdot \vec y \right| \le \| \vec x\| \cdot \| \vec y\|}$
	\\[0.5em]
	\textbf{Mengen:} De Morgan: $\overline{A \capdot B} = \overline{A} \cupplus \overline{B}$ \hfill $\overline{A \cupplus B} = \overline{A} \capdot \overline{B}$
\end{sectionbox}

\begin{sectionbox}
	\subsection[Exp. und Log.]{Exp. und Log.\ \ $e^x := \lim\limits_{n \rightarrow \infty} \left( 1 + \frac{x}{n} \right)^n \hfill e \approx 2,71828$}
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}lll@{}}
		$a^x = e^{x \ln a}$ & $\log_a x = \frac{\ln x}{\ln a}$ & $\ln x \le x -1$\\
		$\ln(x^{a}) = a \ln(x)$ & $\ln(\frac{x}{a}) = \ln x - \ln a$ & $\log(1) = 0$\\
	\end{tabular*}
\end{sectionbox}


\begin{sectionbox}
	\subsection[Matrizen]{Matrizen $\ma A \in\mathbb{K}^{m \times n}$}
	$\ma A=(a_{ij}) \in \mathbb K^{m\times n}$ hat $m$ Zeilen (Index $i$) und $n$ Spalten (Index $j$)
	\begin{tabular*}{\columnwidth}{ll}
	$(\ma A + \ma B)^\top = \ma A^\top + \ma B^\top$ & $(\ma A \cdot \ma B)^\top = \ma B^\top \cdot \ma A^\top$\\
	${(\ma A^\top)}^{-1} = {(\ma A^{-1})}^\top$ & $(\ma A \cdot \ma B)^{-1} = \ma B^{-1}\ma A^{-1}$
	\end{tabular*}
	$\dim \mathbb K = n = \rang\ma A + \dim\ker\ma A$ \qquad $\rang\ma A = \rang\ma A^\top$


	\subsubsection{Quadratische Matrizen $A \in \mathbb{K}^{n \times n}$}
	regulär/invertierbar/nicht-singulär $\Leftrightarrow \det (\ma A) \ne 0 \Leftrightarrow \rang\ma A = n$\\
	singulär/nicht-invertierbar $\Leftrightarrow \det (\ma A) = 0 \Leftrightarrow \rang\ma A \ne n$\\
	orthogonal $\Leftrightarrow \ma A^\top=\ma A^{-1} \Ra \det(\ma A) = \pm 1$\\
	symmetrisch: $\ma A=\ma A^\top$ \qquad schiefsymmetrisch: $\ma A=-\ma A^\top$
	%\item hermitsch: $\ma A=\overline{\ma A}^\top$, unitär:$\ma A^{-1} = \overline{\ma A}^\top$


	\subsubsection[Determinante]{Determinante von $\ma A\in \mathbb K^{n\times n}$: $\det(\ma A)=|\ma A|$}
	$\det\mat{ \ma A & \ma 0 \\ \ma C& \ma D }= \det\mat{ \ma A & \ma B \\ \ma 0 & \ma D } = \det(\ma A)\det(\ma D)$ \\
	\begin{tabular*}{\columnwidth}{@{\extracolsep\fill}ll}
	$\det(\ma A) = \det(\ma A^T)$ & $\det(\ma A^{-1}) = \det(\ma A)^{-1}$
	\end{tabular*}
	$\det(\ma A\ma B) = \det(\ma A)\det(\ma B) = \det(\ma B)\det(\ma A) = \det(\ma B\ma A)$\\
	Hat $\ma A$ 2 linear abhäng. Zeilen/Spalten $\Rightarrow |\ma A|=0$ \\

	\subsubsection{Eigenwerte (EW) $\lambda$ und Eigenvektoren (EV) $\underline v$}
	\begin{emphbox}
		\large $\ma A \vec v = \lambda \vec v$ \quad\ $\det \ma A = \prod \lambda_i$ \quad\ $\Sp \ma A = \sum a_{ii} = \sum \lambda_i$
	\end{emphbox}
	Eigenwerte: $\det(\ma A - \lambda \ma 1) = 0$ Eigenvektoren: $\ker(\ma A - \lambda_i \ma 1) = \vec v_i$\\
	EW von Dreieck/Diagonal Matrizen sind die Elem. der Hauptdiagonale.


	\subsubsection{Spezialfall $2 \times 2$ Matrix $A$}
	\parbox{3cm}{ $\det(\ma A) = ad-bc$ \\ $\Sp(\ma A) = a+d$ } $\mat{a & b\\ c & d}^{-1} = \frac{1}{\det \ma A} \mat{d & -b\\ -c& a}$\\
	$\lambda_{1/2} = \frac{\Sp \ma A}{2} \pm \sqrt{ \left( \frac{\mathrm{sp} \ma A}{2} \right)^2 - \det \ma A }$

	\subsubsection{Differentiation}
	$\frac{\partial \vec x^\top \vec y}{\partial \vec x} = \frac{\partial \vec y^\top \vec x}{\partial \vec x} = \vec y$\qquad
	$\frac{\partial \vec x^\top \ma A \vec x}{\partial \vec x} = (\ma A + \ma A^\top)\vec x$ \\
	$\frac{\partial \vec x^\top \ma A \vec y}{\partial \ma A} = \vec x \vec y^\top$ \qquad $\frac{\partial \det( \ma B \ma A \ma C )}{\partial \ma A} = \det(\ma B \ma A \ma C) \left( \ma A^{-1} \right)^\top$
\end{sectionbox}



\begin{sectionbox}
	\subsubsection{Ableitungsregeln ($\forall \lambda, \mu \in \mathbb R$)}
	\begin{tabular}{@{}l@{\quad}ll@{}}
		Linearität: & $(\lambda f + \mu g)' (x) = \lambda f'(x) + \mu g'(x_0)$  \\
		Produkt: & $(f \cdot g)'(x) = f'(x) g(x) + f(x) g'(x)$\\
		Quotient: & $\left(\frac{f}{g}\right)' (x) = \frac{g(x)f'(x) -f(x) g'(x)}{g(x)^2}$ \quad $\left(\frac{\text{NAZ}-\text{ZAN}}{\text{N}^2}\right)$\\
		Kettenregel & $\left( f\bigl(g(x)\bigr) \right)' = f'\bigl(g(x)\bigr) g'(x)$\\
	\end{tabular}
\end{sectionbox}

\begin{sectionbox}
	\subsection{Integrale $\int e^x\mathrm dx = e^x = (e^x)'$}
	%$\int_a^b f(x) \mathrm dx = F(b) - F(a)$\\
	\begin{tabular*}{\columnwidth}{ll}
	Partielle Integration: & $\int uw'=uw-\int u'w$\\
	Substitution: & $\int f(g(x)) g'(x)\diff x=\int f(t)\diff t$
	\end{tabular*}
	\begin{tablebox}{@{\hspace{5mm}}c@{\extracolsep\fill}c@{\extracolsep\fill}c@{\hspace{5mm}}}
		$F(x) - C$ & $f(x)$ & $f'(x)$ \\ \cmrule
		$\frac{1}{q+1}x^{q+1}$ & $x^q$ & $qx^{q-1}$ \\[1em]
		\raisebox{-0.2em}{$\frac{2\sqrt{ax^3}}{3}$} & $\sqrt{ax}$ & \raisebox{0.2em}{$\frac{a}{2\sqrt{ax}}$}\\
		$x\ln(ax) -x$ & $\ln(ax)$ & $\textstyle \frac{1}{x}$\\
		$\frac{1}{a^2} e^{ax}(ax- 1)$ & $x \cdot e^{ax}$ & $e^{ax}(ax+1)$ \\
		$\frac{a^x}{\ln(a)}$ & $a^x$ & $a^x \ln(a)$ \\
		$-\cos(x)$ & $\sin(x)$ & $\cos(x)$\\
		$\cosh(x)$ & $\sinh(x)$ & $\cosh(x)$\\
		$-\ln |\cos(x)|$ & $\tan(x)$ & $\frac{1}{\cos^2(x)}$ \\
	\end{tablebox}

	\begin{tabular*}{\columnwidth}{ll}
	\multicolumn{2}{c}{$\int e^{at} \sin(bt) \diff t = e^{at} \frac{a \sin(bt) + b \cos(bt)}{a^2 + b^2}$}\\
	$\int \frac{\diff t}{\sqrt{at+b}} = \frac{2 \sqrt{at+b}}{a}$ & $\int t^2 e^{at} \diff t = \frac{(ax-1)^2+1}{a^3} e^{at}$\\
	$\int t e^{at} \diff t = \frac{at-1}{a^2} e^{at}$ & $\int x e^{ax^2} \diff x = \frac{1}{2a} e^{ax^2}$\\
	\end{tabular*}

	\subsubsection{Volumen und Oberfläche von Rotationskörpern um $x$-Achse}
	$V = \pi \int_a^b f(x)^2 \mathrm dx$ \qquad \quad $O = 2 \pi \int_a^b f(x) \sqrt{1 + f'(x)^2} \mathrm dx$
\end{sectionbox}


% Section
% ----------------------------------------------------------------------
\section{Floating-Point Arithmetics}
% ======================================================================


\begin{sectionbox}
	\subsection{IEEE 754}
	Representation of real number $x\in\mathbb{R}$:\\
	
	$x = (-1)^s(1+m2^{-t})2^{e-\beta}$\\

\begin{tabular}{lll}
	Single Precision & 1.8.23 \\
	Double Precision & 1.11.52 \\
\end{tabular}

\begin{tabular}{ll}
Largest finite number & $(2-2^{-t})2^{1-\frac{e_{max}}{2}}$\\
Smallest non zero Number & $2^{1-\frac{e_{max}-1}{2}}$
\end{tabular}\\

\textbf{Accuracy}:\\

$\frac{\left|x_2 - x_1\right|}{\left|x_1\right|} \leq 2^{-t}$\\

\textbf{Machine Epsilon $\epsilon_M$}\\

$\textit{fl}(x) = x(1+\epsilon)$ \hfil $\textit{fl}(x) = \frac{x}{1+\epsilon}$\\
with different $\epsilon$.\\

Maximum error is half the precision given by \\
Machine Epsilon: $\epsilon_M = 2^{-t-1}$.

\textbf{Worst case error}\\
$\rho = r(1+\theta_n)$ with $|\theta_n| \leq \frac{n\epsilon_M}{1-\frac{n}{2}\epsilon_M}$.\\
The number of operations is given by $n$.\\

A more general formulation is:\\
$\rho = r\Pi_{i=1}^{n} (1+\epsilon_i)^{\eta_i}$ with $\eta_i \in \{-1;+1\}$ this results in:\\
$\rho=r(1+\theta_n)$ with $|\theta_n| \leq \gamma_n$ and $\gamma_n = \frac{n\epsilon_M}{1-\frac{n}{2}\epsilon_M}$


\end{sectionbox}
\begin{sectionbox}

	\subsection{Complex Floating Point Arithmetic}
	The error in complex floating point operations can also be represented with:\\
	$x\odot y= (x\circ y)(1+\epsilon)$\\
	
	with $|\epsilon|\leq \alpha \epsilon_M$. The alpha coefficients are given in the table below:\\
	
		\includegraphics[width=\textwidth]{img/cplx_fl_op.png}

\end{sectionbox}
	\section{Linear Algebra}
\begin{sectionbox}
\subsection{SVD}
A matrix is unitary if: $\mathbf{U}^H\mathbf{U}=\mathbf{U}\mathbf{U}^H =\mathbf{I}$\\

The singular decomposition (SVD) of the matrix $\mathbf{A}\in\mathbb{C}^{mxn}$ reads as:\\
$\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^H$ where:\\
$\mathbf{U}$ and $\mathbf{V}$ are unitary and $\mathbf{\Sigma}\in\mathbb{C}^{mxn}$ is diagonal.\\

$\mathbf{\Sigma}_0 = \text{diag}(\sigma_1, \sigma_2, ...,\sigma_{\text{min}(m,n)})$

$\mathbf{\Sigma} = \begin{cases}
	\mathbf{\Sigma}_0, \text{for } m=n\\
	[\mathbf{\Sigma}_0, \mathbf{0}], \text{for } m < n \text{ (wide matrix)}\\
	\begin{bmatrix}
		\mathbf{\Sigma}_0,\\
		\mathbf{0}
	\end{bmatrix}, \text{for } m > n \text{ (tall matrix)}
\end{cases}$\\

The \textbf{SVD} can also be represented in the following form:\\

$\mathbf{A} = \sum_{i=1}^{\text{min(m,n)}} \sigma_i \mathbf{u}_i\mathbf{v}_i^H$\\

with\\
$\mathbf{U} = [\mathbf{u}_1,\mathbf{u}_2,...\mathbf{u}_m] \in\mathbb{C}^{mxm}$\\
$\mathbf{V} = [\mathbf{v}_1, \mathbf{v}_2, ...\mathbf{v}_n] \in\mathbb{C}^{nxn}$\\

The rank of $r=rank(\mathbf{A})$ is given by the number of \textbf{non zero} singular values $\sigma_i>0$\\
$range(\mathbf{A}) = span(\mathbf{u}_1,...\mathbf{u}_r)$\\
$null(\mathbf{A}) = span(\mathbf{v}_{r+1},  ...\mathbf{v}_n)$\\

Removing the singular values that are zero $\sigma_{r+1}=...=\sigma_{n}=0$ the reduced SVD can be expressed with:\\

$\mathbf{A} = \mathbf{U}_{red}\mathbf{\Sigma}_{red}\mathbf{V}^H_{red}$\\
with\\
$\mathbf{\Sigma}_{red} = \text{diag}(\sigma_1, ...\sigma_r)$, 
$\mathbf{U}_{red} = [\mathbf{u}_1, ...\mathbf{u}_r]$ and\\
$\mathbf{V}_{red} = [\mathbf{v}_1, ...\mathbf{v}_r]$\\

\textbf{Gram Matrix}\\

$\mathbf{B} = \mathbf{A}\mathbf{A}^H$ is Hermitian by definition. It's SVD reads as:\\

$\mathbf{B} = \mathbf{U}_{red}\mathbf{\Sigma}_{red}\mathbf{U}_{red}^H$\\
A similar discussion is possible for $\mathbf{C} = \mathbf{A}^H\mathbf{A}$:\\

$\mathbf{C}=\mathbf{A}^H\mathbf{A} = \mathbf{V}_{red}\mathbf{\Sigma}_{red}\mathbf{V}^H_{red}$\\

Both decompositions are like the (reduced) EVD.\\

\end{sectionbox}
\begin{sectionbox}
\subsection{Projectors}
Projectors are idempotent matrices. $\mathbf{P} \in \mathbb{C}^{mxm}$ is a projector if:\\
$$\mathbf{P}^2 = \mathbf{P}$$

\textbf{Orthogonal projectors} are additionally Hermitian. Therefore:\\
$$\mathbf{P}^2 = \mathbf{P} \text{ and } \mathbf{P}^H=\mathbf{P}$$

is a orthogonal projector.\\

The orthogonal complement of a projector $\mathbf{P}$ is defined as:\\
$\mathcal{S}_P^\perp = \{\mathbf{v}: (\mathbf{I}-\mathbf{P})\mathbf{v}=\mathbf{v}\}$\\

The eigenvalues of a projector are out of the set $\lambda_i\in\{1;0\}$. And therefore:

$$\mathbf{P} = \mathbf{U}\text{diag}(1,...,1,0,...0)\mathbf{U}^H$$

with reads in the reduced form:
$$\mathbf{P} = \mathbf{U}_{red}\mathbf{U}_{red}^H = \sum_{i=1}^{r}\mathbf{u}_i\mathbf{u}_i^H$$
with $\mathbf{U}_{red} = [\mathbf{u}_1,...,\mathbf{u}_r]$. 
Furthermore it can be concluded that:

$$rank(\mathbf{P}) = tr(\mathbf{P}) =r$$.
\end{sectionbox}
\begin{sectionbox}
\subsubsection{Projectors of vectors}
We can construct a projector of a vector $\mathbf{a}$ according to:
$$\mathbf{P}_a = \frac{\mathbf{a}\mathbf{a}^H}{\parallel\mathbf{a}\parallel^2_2}$$

The projector onto the orthogonal complement is given by:
$$\mathbf{P}^\perp_\mathbf{a} = \mathbf{I}-\mathbf{P}_a = \mathbf{I} - \frac{\mathbf{a}\mathbf{a}^H}{\parallel\mathbf{a}\parallel^2_2}$$
\includegraphics[width=\textwidth]{img/vector_decomp.png}

Given orthogonal vectors $\mathbf{a}_1, ... \mathbf{a}_n$ we can perform multiple projections onto the orthogonal complement via:\\
$$\prod_{i=1}^n \mathbf{P}_{\mathbf{a}_i}^\perp = \mathbf{I} - \sum_{i=1}^{n}\frac{\mathbf{a}_i\mathbf{a}_i^H}{\parallel\mathbf{a}_i\parallel^2_2}$$
\end{sectionbox}
\begin{sectionbox}
\subsection{Matrix and Vector norms}
\textbf{Vector norms}:\\

\begin{tabular*}{\columnwidth}{ll}
	p-norm: & $\parallel\mathbf{x}\parallel_p = \sqrt[p]{\sum_{i=1}^{n} |x_i|^p}$\\
	1-norm: & $\parallel\mathbf{x}\parallel_1 = \sum_{i=1}^{n} |x_i|$\\
	2-norm: & $\parallel\mathbf{x}\parallel_2 = \sqrt{\sum_{i=1}^{n} |x_i|^2}$\\
	$\infty$-norm & $\parallel\mathbf{x}\parallel_\infty = \max_{i\in\{1,...,n\}} |x_i|$\\
\end{tabular*}\\

\textbf{Matrix norms}:\\
$\parallel \mathbf{A}\parallel \leq \parallel \mathbf{A}\parallel \parallel\mathbf{B}\parallel$
The columnwise vectorization of $\mathbf{A}$ reads as:\\
$\text{vec}(\mathbf{A}) = [\mathbf{a}_1^T, ...,\mathbf{a}_n^T]^T\in\mathbb{C}^{mxn}$\\

$$\parallel\mathbf{A}\parallel_F = \parallel vec(A)\parallel_2 = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n} |a_{ij}|^2}$$
\end{sectionbox}
\begin{sectionbox}
\subsection{Householder Reflection}
With the Householder Reflection we want remove all elements of a vector except the first one. Give a vector $\mathbf{x}$:\\
$\mathbf{x} = \begin{bmatrix}
	x_1\\
	\mathbf{x}_2
\end{bmatrix}\in\mathbb{C}^m$\\

With the unitary matrix $\mathbf{F}\in\mathbb{C}^{mxm}$ ($\mathbf{F}^H\mathbf{F}=\mathbf{F}\mathbf{F}^H = \mathbf{I}$)we want to achieve the following:\\
$$\mathbf{F}\mathbf{x} = \begin{bmatrix}
	\alpha\\
	0\\
	\vdots\\
	0
\end{bmatrix} = \alpha \mathbf{e}_1$$\\
It follows: $\parallel\mathbf{F}\mathbf{x}\parallel_2^2 = \mathbf{x}^H\mathbf{F}^H\mathbf{F}\mathbf{x} = \parallel \mathbf{x}\parallel_2^2$ and therefore:\\
$|\alpha| = \parallel \mathbf{x}\parallel_2^2$\\
The overall goal is to find the matrix $\mathbf{F}$ that achieves:
$$\mathbf{F}\mathbf{x} = \begin{bmatrix}
	\parallel \mathbf{x}\parallel_2 e^{j\phi}\\
	0\\
	\vdots\\
	0
\end{bmatrix} = \parallel \mathbf{x}\parallel_2 e^{j\phi} \mathbf{e}_1$$\\
We want to find the vector\\

$\mathbf{v} = \parallel\mathbf{x}\parallel_2 e^{j\phi}\mathbf{e}_1 - \mathbf{x}$\\

The vectors $\mathbf{x}$, $\mathbf{v}$ and $\parallel\mathbf{x}\parallel_2 e^{j\phi}\mathbf{e}_1$ form an isosceles triangle. The height vector is given by the projection onto the orthogonal complement of $\mathbf{v}$:

$\mathbf{h} = \mathbf{P}^\perp_\mathbf{v}\mathbf{x} = (\mathbf{I} - \mathbf{P}_\mathbf{v})\mathbf{x} = \mathbf{x} - \frac{\mathbf{v}\mathbf{v}^H}{\parallel\mathbf{v}\parallel_2^2} \mathbf{x}$

\includegraphics[width=\textwidth]{img/householder.png}

The matrix $\mathbf{F}$ is given by:
$$\mathbf{F} = \mathbf{I} - 2\frac{\mathbf{v}\mathbf{v}^H}{\parallel\mathbf{v}\parallel_2^2}$$

Decomposing the Householder reflector into it's EVD leads to:

$$[\mathbf{w}, \mathbf{W}_\perp]\text{diag}(-1,1,...,1)[\mathbf{w},\mathbf{W}_\perp]^H$$\\
 
where $\mathbf{w} = \frac{\mathbf{v}}{\parallel \mathbf{v}\parallel_2}$ and $\mathbf{P}_\mathbf{v}^\perp = \mathbf{W}_perp\mathbf{W}_\perp^H$\\
Only $\lambda_1=-1$ of $\mathbf{F}$ all other eigenvalues are $\lambda_i=1$.
\end{sectionbox}

\begin{sectionbox}
\subsubsection{Choice of Phase after Reflection}

A possible choice is: $\phi=0$ such that $\alpha$ is positive-real.
An other goal is to make $\mathbf{v}$ as long as possible therefore:
$$\phi_{opt} = \text{argmax}_{\phi} = \parallel \mathbf{v}\parallel_2^2$$
with $\mathbf{x} = [x_1, \mathbf{x}_2^T]^T$ and \\
$\mathbf{v} = \parallel \mathbf{x}\parallel_2 e^{j\phi}\mathbf{e}_1 - \mathbf{x}$ the objective function can be expressed as
$$\parallel \mathbf{v} \parallel_2^2 = |e^{j\phi}\parallel\mathbf{x}\parallel_2 - x_1|^2 + \parallel \mathbf{x}_2 \parallel_2^2$$
As the second term is independent of $\phi$, the first term must be maximized. To this end, it is necessary to choose $\phi_{opt}=\psi+\pi$ for $x_1 = |x_1|e^{j\psi}$, resulting in
$$\mathbf{v} = \begin{bmatrix}
	-(\parallel\mathbf{x}\parallel_2 + |x_1|)e^{j\psi}\\
	-\mathbf{x}_2
\end{bmatrix}$$
The maximum objective function is:
$$\parallel\mathbf{v}_{opt}\parallel_2^2 = 2\parallel\mathbf{x}_2\parallel_2^2 + 2|x_1|\parallel\mathbf{x}\parallel_2$$

\end{sectionbox}
\section{QR Decomposition}
\begin{sectionbox}
For a full-rank and tall matrix $\mathbf{A} = [\mathbf{a}_1, ...,\mathbf{a}_n] \in \mathbb{C}^{mxn}$ with $m\gg n$ the QR decomposition reads as:
$$\mathbf{A} = \mathbf{Q}\mathbf{R}$$
where $\mathbf{Q}\in\mathbb{C}^{mxm}$ is unitary and
$$\mathbf{R}=\begin{bmatrix}
\mathbf{R}_{red}\\
\mathbf{0}
\end{bmatrix} \in \mathbb{C}^{mxn}$$
with
$$\mathbf{R}_{red} = \begin{bmatrix}
r_{11} & r_{12} & \cdots & r_{1n}\\
0 & r_{22} & \cdots & r_{2n}\\
0 & 0 & \ddots & \vdots \\
0 & \cdots & r_{nn}
\end{bmatrix} \in \mathbb{C}^{nxn}$$

with ($\nu \leq n$):\\
$span(\mathbf{a}_1, ...,\mathbf{a}_\nu) = span(\mathbf{q}_1, ...,\mathbf{q}_\nu)$\\
which results in:\\
$range(\mathbf{A}) = span(\mathbf{a}_1, ...,\mathbf{a}_n) = span(\mathbf{q}_1, ...,\mathbf{q}_n) = range(\mathbf{Q}_{red})$\\
$\mathbf{Q}_{red}$ is an orthonormal basis for the range space of $\mathbf{A}$. \\

$\mathbf{Q}_{red} = [\mathbf{q}_1, ...,\mathbf{q}_n]\in \mathbb{C}^{mxn}$

$\mathbf{Q}^\perp_{red} = [\mathbf{q}_{n+1}, ... ,\mathbf{q}_m] \in \mathbb{C}^{mxm-n}$\\
It follows from 
$\mathbf{Q} = [\mathbf{Q}_{red},\mathbf{Q}_{red}^\perp]$ that\\
$$\mathbf{I} = \mathbf{Q}^H\mathbf{Q} = $$
$$\begin{bmatrix}
	\mathbf{Q}_{red}^H\\
	\mathbf{Q}_{red}^{\perp,H}
\end{bmatrix} \cdot [\mathbf{Q}_{red}, \mathbf{Q}_{red}^\perp] = \begin{bmatrix}
\mathbf{Q}_{red}^H\mathbf{Q}_{red} & \mathbf{Q}_{red}^H\mathbf{Q}_{red}^\perp \\
\mathbf{Q}_{red}^{\perp,H}\mathbf{Q}_{red} & \mathbf{Q}_{red}^{\perp,H}\mathbf{Q}_{red}^\perp
\end{bmatrix}$$\\

Therefor: $\mathbf{Q}_{red}^H\mathbf{Q}_{red}^\perp = 0$. Consequently the space spanned by the columns of $\mathbf{Q}_{red}^\perp$ is perpendicular to $range(\mathbf{Q}_{red})$. In other words:
$$null(\mathbf{A}^H) = range(\mathbf{Q}_{red}^\perp)$$\\
that is, the matrix $\mathbf{Q}_{red}^\perp$ is a basis for the left nullspace of $\mathbf{A}$.

\end{sectionbox}
\begin{sectionbox}
\subsection{Operation Count of basic operations}
\begin{tablebox}{@{\hspace{5mm}}c@{\extracolsep\fill}c@{\extracolsep\fill}c@{\hspace{5mm}}}
	$f$ & Mult.+Sum. & \#FLOPS \\ \cmrule
	$\mathbf{a}^H\mathbf{b} = \sum_{i=1}^{m}a_i^*b_i$ & $m$+$m-1$ & $2m-1$ \\
	$\parallel\mathbf{a}\parallel_2 = \sqrt{\sum_{i=1}^{m} a_i^*a_i}$ & $m+m-1+1$ & $2m$\\
	$\gamma \mathbf{a}$ & $m$ & $m$\\
	$\mathbf{a}\pm\mathbf{b}$ & $m$ & $m$
	
\end{tablebox}



\end{sectionbox}
\begin{sectionbox}
\subsection{Classical Gram-Schmidt Orthogonalization}
\includegraphics[width=\textwidth]{img/classic_gram_schmidt.png}

\end{sectionbox}

\begin{sectionbox}
\subsection{Modified Gram-Schmidt Orthogonalization}
\includegraphics[width=\textwidth]{img/modified_gram_schmidt}

\includegraphics[width=\textwidth]{img/classic_gram_schmidt2.png}

\textbf{Operation Count}: \#FLOPS $=\mathcal{O}(2mn^2)$
\end{sectionbox}
\begin{sectionbox}
\subsection{Househodler QR Decomposition}

With the Householder reflector $\mathbf{Q}_1 = \mathbf{F}_1$ the first column of matrix $\mathbf{A}$ is transformed as follows
$$\mathbf{Q}_1 \mathbf{A} = \begin{bmatrix}
\times & \times & \times \\
0 & \times & \times \\
0 & \times & \times \\
0 & \times & \times \\
0 & \times & \times
\end{bmatrix}$$
In the next step we apply the matrix $\mathbf{Q}_2$ which inherits a Householder reflector $\mathbf{F}_2\in\mathbb{C}^{m-1\times m-1}$
$$\mathbf{Q}_2 = \begin{bmatrix}
1 & \mathbf{0}^T \\
\mathbf{0} & \mathbf{F}_2
\end{bmatrix} \in \mathbb{C}^{m \times m}$$
After that the matrix $\mathbf{Q}_2\mathbf{Q}_1\mathbf{A}$ looks like this
\end{sectionbox}
\begin{sectionbox}
$$\mathbf{Q}_2\mathbf{Q}_1 \mathbf{A} = \begin{bmatrix}
	\times & \times & \times \\
	0 & \times & \times \\
	0 & 0 & \times \\
	0 & 0 & \times \\
	0 & 0 & \times
\end{bmatrix}$$
The unitary rotation matrix $\mathbf{Q}$ at step $i\in[1,...,n]$ is given by
$$\mathbf{Q}_i = \begin{bmatrix}
\mathbf{I}_{i-1} & \mathbf{0}\\
\mathbf{0} & \mathbf{F}_i
\end{bmatrix}\in\mathbb{C}^{m\times m}$$
with $\mathbf{F}_i = \mathbf{I}-2\frac{\mathbf{v_i}\mathbf{v_i}^H}{\parallel \mathbf{v}\parallel_2^2} \in \mathbb{C}^{m-i+1 \times m-i+1}$\\

After rotating all $n$ columns of $\mathbf{A}$ we generate the QR decomposition with

$\mathbf{A} = \mathbf{Q}_1^H\mathbf{Q}_2^H\cdots\mathbf{Q}_n^H \mathbf{R} = \mathbf{Q}\mathbf{R}$\\

$\mathbf{R} \in \mathbb{C}^{mxn}$\\
Furthermore it holds\\
$\mathbf{Q} = \mathbf{Q}_1^H\mathbf{Q}_2^H\cdots\mathbf{Q}_n^H = \mathbf{Q}_1\mathbf{Q}_2\cdots\mathbf{Q}_n$ 
since $\mathbf{Q}_i$ is Hermitian.

\end{sectionbox}

\begin{sectionbox}
\includegraphics[width=\textwidth]{img/householder_QR.png}
\textbf{Operation Count Algorithm $4$}: \#FLOPS $=\mathcal{O}(2mn^2 - \frac{2}{3}n^3)$
\end{sectionbox}
\begin{sectionbox}

\includegraphics[width=\textwidth]{img/householder_QR_further.png}\\

\textbf{Operation Count Algorithm $5$}: \#FLOPS $=\mathcal{O}(2mn^2-\frac{2}{3}n^3)$\\

\textbf{Operation Count Algorithm $6$}: \#FLOPS $=\mathcal{O}(4mn-2n^2)$\\

\includegraphics[width=\textwidth]{img/householder_QR_Qperp.png}\\

\textbf{Operation Count Algorithm $7$}: \#FLOPS $=\mathcal{O}(4m^2n+2n^3-6mn^2)$\\
If $m=n$ the number of FLOPS for Algorithm $7$ is zero, because the partition $\mathbf{Q}_red^\perp$ does not exist in this case.
\end{sectionbox}
\begin{sectionbox}
	\subsubsection{Householder for null($\mathbf{A}^H$)}
	
In the case, that a basis for the nullspace $\mathbf{A}^H$, i.e., the orthogonal complement of $range(A)$ must be found Algorithm $7$ can be employed. null$(\mathbf{A}^H)=$range$(\mathbf{Q}_{red}^H)$

$$\mathbf{Q} = [\mathbf{Q}_{red}, \mathbf{Q}_{red}^\perp]$$

\end{sectionbox}
\section{Back Substitution}
\begin{sectionbox}
Solving an equation system with an upper triangular $\mathbf{R}$\\
$\mathbf{R}\mathbf{x} = \mathbf{b}$\\
can be solved with Algorithm $8$.


$$\mathbf{R}= \begin{bmatrix}
r_{11} & r_{12} & \cdots r_{1n} \\
0 & r_{22} &\\
& \ddots & \vdots\\
& & r_{nn} & 
\end{bmatrix}\in\mathbb{C}^{n\times n}$$\\

\includegraphics[width=\textwidth]{img/algo8_backsub.png}\\
\textbf{Operation Count Algorithm $8$}: \#FLOPS $=\mathcal{O}(n^2)$\\
\end{sectionbox}

\begin{sectionbox}
\includegraphics[width=\textwidth]{img/algo9_R_inverse.png}\\
\textbf{Operation Count Algorithm $9$}: \#FLOPS $=\mathcal{O}(\frac{1}{3}n^3)$\\
\end{sectionbox}

\section{Least Squares}
\begin{sectionbox}
\subsection{Least Square for Full-Rank Matrix}

\end{sectionbox}
\begin{sectionbox}
	\subsection{Rank-Deficient Least Square}
	
\end{sectionbox}
% ======================================================================
% End
% ======================================================================
\end{document}
